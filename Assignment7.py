# -*- coding: utf-8 -*-
"""Assignment7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AclJ5zcnT9VczKLkSW14T4O2aKiSPuTe
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('average_perceptron_tagger')

text="Tokenization is the first step in text analytics. The process of breaking down a text"
from nltk.tokenize import sent_tokenize
tokenized_text=sent_tokenize(text)
tokenized_text

from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
tokenized_word

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))

stop_words

text="How to remove stop words with NLTK library in python?"
import re
text= re.sub('[^a-zA-Z]', ' ', text)
text

tokens=word_tokenize(text.lower())
tokens

filtered_text=[]
for w in tokens:
  if w not in stop_words:
    filtered_text.append(w)

filtered_text

from nltk.stem import PorterStemmer
e_words=["wait","waiting","waited","waits"]
ps=PorterStemmer()
for w in e_words:
  rootWord=ps.stem(w)
  print(rootWord)

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print("Lemma for {} is {}" .format(w,wordnet_lemmatizer.lemmatize(w)))